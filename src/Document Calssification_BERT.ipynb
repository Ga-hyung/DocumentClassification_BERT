{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cf7a9b8-9da8-4bd9-b591-b22256b17ff3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.7.0-cp38-cp38-win_amd64.whl (430.8 MB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.37.0)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-12.0.0-py2.py3-none-win_amd64.whl (13.1 MB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.36.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.6.0-cp38-cp38-win_amd64.whl (2.8 MB)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.17.2)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.21.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.6.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Installing collected packages: tensorboard-data-server, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-2.0 gast-0.4.0 google-pasta-0.2.0 h5py-3.6.0 keras-preprocessing-1.1.2 libclang-12.0.0 opt-einsum-3.3.0 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.23.1 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f76b77-0f2d-4fa2-a8da-e88fe77d5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import textwrap\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3daded9-faf9-42d4-b27a-627c551b9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('../data/TypeStress.xlsx', sheet_name = '1316')\n",
    "data = data[['Municode', 'year', 'Name', 'County', 'TypeofStress']]\n",
    "data_2 = pd.read_excel('../data/TypeStress.xlsx', sheet_name = '1719')\n",
    "data_2 = data_2[['Entity Name', 'County', 'year', 'Municode', 'Type of Stress']]\n",
    "data_2.rename(columns = {'Entity Name': 'County', 'Type of Stress': 'TypeofStress', 'County': 'Name'}, inplace=True)\n",
    "city_types = pd.concat([data, data_2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0835adf-3ff0-4bac-8b2c-d1f40b68361d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Municode</th>\n",
       "      <th>year</th>\n",
       "      <th>Name</th>\n",
       "      <th>County</th>\n",
       "      <th>TypeofStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50203000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>Cayuga</td>\n",
       "      <td>Inconclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550262000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>Yonkers</td>\n",
       "      <td>Westchester</td>\n",
       "      <td>Inconclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>330245000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>Port Jervis</td>\n",
       "      <td>Orange</td>\n",
       "      <td>Inconclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380247000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>Rensselaer</td>\n",
       "      <td>Rensselaer</td>\n",
       "      <td>Inconclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>330245000000</td>\n",
       "      <td>2015</td>\n",
       "      <td>Port Jervis</td>\n",
       "      <td>Orange</td>\n",
       "      <td>Inconclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>550261000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>Westchester</td>\n",
       "      <td>City of White Plains</td>\n",
       "      <td>No Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>550261000000</td>\n",
       "      <td>2019</td>\n",
       "      <td>Westchester</td>\n",
       "      <td>City of White Plains</td>\n",
       "      <td>No Designation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>550262000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>Westchester</td>\n",
       "      <td>City of Yonkers</td>\n",
       "      <td>Susceptible Fiscal Stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>550262000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>Westchester</td>\n",
       "      <td>City of Yonkers</td>\n",
       "      <td>Moderate Fiscal Stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>550262000000</td>\n",
       "      <td>2019</td>\n",
       "      <td>Westchester</td>\n",
       "      <td>City of Yonkers</td>\n",
       "      <td>No Designation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Municode  year         Name                County  \\\n",
       "0     50203000000  2013       Auburn                Cayuga   \n",
       "1    550262000000  2013      Yonkers           Westchester   \n",
       "2    330245000000  2014  Port Jervis                Orange   \n",
       "3    380247000000  2014   Rensselaer            Rensselaer   \n",
       "4    330245000000  2015  Port Jervis                Orange   \n",
       "..            ...   ...          ...                   ...   \n",
       "118  550261000000  2018  Westchester  City of White Plains   \n",
       "119  550261000000  2019  Westchester  City of White Plains   \n",
       "120  550262000000  2017  Westchester       City of Yonkers   \n",
       "121  550262000000  2018  Westchester       City of Yonkers   \n",
       "122  550262000000  2019  Westchester       City of Yonkers   \n",
       "\n",
       "                  TypeofStress  \n",
       "0                 Inconclusive  \n",
       "1                 Inconclusive  \n",
       "2                 Inconclusive  \n",
       "3                 Inconclusive  \n",
       "4                 Inconclusive  \n",
       "..                         ...  \n",
       "118             No Designation  \n",
       "119             No Designation  \n",
       "120  Susceptible Fiscal Stress  \n",
       "121     Moderate Fiscal Stress  \n",
       "122             No Designation  \n",
       "\n",
       "[367 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7930492-6eb2-4b3a-aa73-d7f705ee43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 종류 numeric label 부여하기\n",
    "target = []\n",
    "for idx, city in city_types.iterrows():\n",
    "    if city['TypeofStress'] == 'No Designation':\n",
    "        target.append(0)\n",
    "    elif city['TypeofStress'] == 'Susceptible Fiscal Stress':\n",
    "        target.append(1)\n",
    "    elif city['TypeofStress'] == 'Moderate Fiscal Stress':\n",
    "        target.append(2)\n",
    "    elif city['TypeofStress'] == 'Significant Fiscal Stress':\n",
    "        target.append(3)\n",
    "    else:\n",
    "        target.append('nan')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d73c75f-041f-4a17-b3a2-fddbdc583254",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_types['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9727eb9a-9180-4881-9c4f-735395437cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    }
   ],
   "source": [
    "# Target 데이터가 nan가 아닌 데이터 갯수\n",
    "print(len(city_types[city_types['target']!='nan']))\n",
    "new_city_types = city_types[city_types['target']!='nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6f51f0b6-3136-49dc-a321-483925e213cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Municode</th>\n",
       "      <th>year</th>\n",
       "      <th>Name</th>\n",
       "      <th>County</th>\n",
       "      <th>TypeofStress</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>10201000000</td>\n",
       "      <td>2016</td>\n",
       "      <td>Albany</td>\n",
       "      <td>Albany</td>\n",
       "      <td>Significant Fiscal Stress</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>10201000000</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>Albany</td>\n",
       "      <td>Significant Fiscal Stress</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10201000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>Albany</td>\n",
       "      <td>Albany</td>\n",
       "      <td>No Designation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10209000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>Albany</td>\n",
       "      <td>City of Cohoes</td>\n",
       "      <td>No Designation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10201000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>Albany</td>\n",
       "      <td>City of Albany</td>\n",
       "      <td>Moderate Fiscal Stress</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10201000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>Albany</td>\n",
       "      <td>City of Albany</td>\n",
       "      <td>Susceptible Fiscal Stress</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10209000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>Albany</td>\n",
       "      <td>City of Cohoes</td>\n",
       "      <td>No Designation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>10201000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>Albany</td>\n",
       "      <td>Albany</td>\n",
       "      <td>Significant Fiscal Stress</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>10260000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>Albany</td>\n",
       "      <td>City of Watervliet</td>\n",
       "      <td>Significant Fiscal Stress</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>10260000000</td>\n",
       "      <td>2018</td>\n",
       "      <td>Albany</td>\n",
       "      <td>City of Watervliet</td>\n",
       "      <td>Susceptible Fiscal Stress</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>50203000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>Cayuga</td>\n",
       "      <td>No Designation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>50203000000</td>\n",
       "      <td>2015</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>Cayuga</td>\n",
       "      <td>No Designation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>50203000000</td>\n",
       "      <td>2016</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>Cayuga</td>\n",
       "      <td>No Designation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>180204000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>Batavia</td>\n",
       "      <td>Genesee</td>\n",
       "      <td>No Designation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>180204000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>Batavia</td>\n",
       "      <td>Genesee</td>\n",
       "      <td>No Designation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Municode  year     Name              County  \\\n",
       "222   10201000000  2016   Albany              Albany   \n",
       "220   10201000000  2015   Albany              Albany   \n",
       "24    10201000000  2013   Albany              Albany   \n",
       "20    10209000000  2018   Albany      City of Cohoes   \n",
       "0     10201000000  2017   Albany      City of Albany   \n",
       "1     10201000000  2018   Albany      City of Albany   \n",
       "19    10209000000  2017   Albany      City of Cohoes   \n",
       "218   10201000000  2014   Albany              Albany   \n",
       "115   10260000000  2017   Albany  City of Watervliet   \n",
       "116   10260000000  2018   Albany  City of Watervliet   \n",
       "73    50203000000  2014   Auburn              Cayuga   \n",
       "113   50203000000  2015   Auburn              Cayuga   \n",
       "153   50203000000  2016   Auburn              Cayuga   \n",
       "41   180204000000  2013  Batavia             Genesee   \n",
       "82   180204000000  2014  Batavia             Genesee   \n",
       "\n",
       "                  TypeofStress target  \n",
       "222  Significant Fiscal Stress      3  \n",
       "220  Significant Fiscal Stress      3  \n",
       "24              No Designation      0  \n",
       "20              No Designation      0  \n",
       "0       Moderate Fiscal Stress      2  \n",
       "1    Susceptible Fiscal Stress      1  \n",
       "19              No Designation      0  \n",
       "218  Significant Fiscal Stress      3  \n",
       "115  Significant Fiscal Stress      3  \n",
       "116  Susceptible Fiscal Stress      1  \n",
       "73              No Designation      0  \n",
       "113             No Designation      0  \n",
       "153             No Designation      0  \n",
       "41              No Designation      0  \n",
       "82              No Designation      0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_city_types.sort_values(by ='Name')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5169b3ba-d937-4fc5-b142-3b35ad4a6e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '../data/MDNA_txt/'\n",
    "temp = []\n",
    "file_list = os.listdir(path)\n",
    "for file in file_list:\n",
    "    city = re.sub('[^a-z|A-Z]|txt', '', file)\n",
    "#     print(city)\n",
    "    year = int(re.findall('\\d+',file)[0])\n",
    "#     print(year)\n",
    "    municode = str(int(re.findall('\\d+', file)[1]))\n",
    "#     print(municode)\n",
    "    city_lookup = new_city_types[new_city_types['Name'] == city]\n",
    "    subset = city_lookup[city_lookup['year'] == year]\n",
    "    subset = subset[subset['Municode'].apply(lambda x: str(x)[:len(municode)]) == municode]\n",
    "#     print(subset)\n",
    "    if subset.empty == False:\n",
    "        label = int(subset['target'])\n",
    "        temp.append([file, label])\n",
    "#         shutil.move(path+file, destination+f\"{label}/\")\n",
    "df = pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7c8df236-6f01-4d13-b2f1-a9d432094ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    71\n",
       "1     8\n",
       "2     5\n",
       "3     4\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터에 사용되는 파일들의 dataframe\n",
    "df[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ebc6880d-7866-4c5e-bec1-a6abb09f70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data인 text 데이터를 불러온 dataframe\n",
    "temp = []\n",
    "for idex, data in df.iterrows():\n",
    "    with open(f'../data/MDNA_txt/{str(data[0])}', 'rt',encoding = 'unicode_escape') as f:\n",
    "        a = f.read()\n",
    "        a = re.sub(r'&amp;', '&',a)\n",
    "        a = re.sub(r'(@.*?)[\\s]', ' ', a)\n",
    "        a = re.sub(r'\\n', '', a)\n",
    "        a = re.sub(r'\\t', '', a)\n",
    "    temp.append([a, data[1]])\n",
    "data = pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d7f3cb40-2e8d-4855-883a-b715fabde2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Management Discussion &amp; Analysis   OVERVIEW ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Management Discussion &amp; AnalysisOVERVIEWThe Ma...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Management Discussion &amp; Analysis   OVERVIEW  T...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Management Discussion &amp; Analysis   OVERVIEW  T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOR THE YEAR ENDED JUNE 30, 2014    Our discus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>REQUIRED SU PPL EM ENTA RY IN FORMATIONMANAGEM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>As management of the Cir of Watertown (the \"Ci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>City of Yonkers, New YorkManagement's Discussi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>City of Yonkers, New YorkManagement's Discussi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>City of Yonkers,  New YorkManagement's Discuss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  1\n",
       "0     Management Discussion & Analysis   OVERVIEW ...  3\n",
       "1   Management Discussion & AnalysisOVERVIEWThe Ma...  3\n",
       "2   Management Discussion & Analysis   OVERVIEW  T...  3\n",
       "3   Management Discussion & Analysis   OVERVIEW  T...  1\n",
       "4   FOR THE YEAR ENDED JUNE 30, 2014    Our discus...  0\n",
       "..                                                ... ..\n",
       "83  REQUIRED SU PPL EM ENTA RY IN FORMATIONMANAGEM...  0\n",
       "84  As management of the Cir of Watertown (the \"Ci...  0\n",
       "85  City of Yonkers, New YorkManagement's Discussi...  2\n",
       "86  City of Yonkers, New YorkManagement's Discussi...  2\n",
       "87  City of Yonkers,  New YorkManagement's Discuss...  1\n",
       "\n",
       "[88 rows x 2 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d8f8a84b-5c9d-43f0-ac1d-b8157d04fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f4b0c429-c75e-4123-997e-82e85b4d6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_long = []\n",
    "for idx, text in data.iterrows():\n",
    "    txt = text[0]\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    if len(tokens)>=512:\n",
    "        doc_long.append(idx)\n",
    "        \n",
    "# len(doc_long) 85\n",
    "# 모든 document가 512개의 token 보다 크다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ea5adfbd-7e3b-44f6-b092-1d6559cf0739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number words:  3527\n",
      "number of sentences 103\n"
     ]
    }
   ],
   "source": [
    "# remove all double space\n",
    "text = data[0][1]\n",
    "text = text.replace('  ', ' ')\n",
    "num_words = len(text.split())\n",
    "print(\"number words: \", num_words)\n",
    "num_sens = text.count('. ')\n",
    "print(\"number of sentences\", num_sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4a7c61b-914d-4231-9ca3-5187fe6da33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1)\n",
    "train, test = data[:60], data[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e106e132-979c-436f-9d50-eb4a2d0a5062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0 comments\n",
      "Read 10 comments\n",
      "Read 20 comments\n",
      "Read 30 comments\n",
      "Read 40 comments\n",
      "Read 50 comments\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "length = []\n",
    "\n",
    "for idx, text in train.iterrows():\n",
    "    if ((len(input_ids)%10)==0):\n",
    "        print('Read {:,} comments'.format(len(input_ids)))\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens = True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    length.append(len(encoded_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efa9b095-f996-4709-b7ec-172f06964225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length : 722 tokens\n",
      "Max length : 7,660 tokens\n",
      "Median length : 4,391.0 tokens\n"
     ]
    }
   ],
   "source": [
    "print('Min length : {:,} tokens'.format(min(length)))\n",
    "print('Max length : {:,} tokens'.format(max(length)))\n",
    "print('Median length : {:,} tokens'.format(np.median(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b221f278-b623-4e2d-a874-4d1f9171b83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '# of documents')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAFNCAYAAABmNpkJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyrUlEQVR4nO3de1iUdf7/8dfADCCCW/gFdLW0urS03UXbLFHT0hQVkE0tj1vmIWtb3Wx38RBl2kEzV81DrlpbmbZGlrq6ZfpNs69Ku2atYt9Mvy0WggsIqSACw8zn90dX84NQBwhm4Ob5uK6uy/v8vt/N3L68D3PbjDFGAAAAaNQC/F0AAAAAfjxCHQAAgAUQ6gAAACyAUAcAAGABhDoAAAALINQBAABYAKEOgM+dPHlSnTp1UlJSkpKSkpSYmKihQ4dq8+bN/i6tVt566y2tX7++yviTJ0+qa9eu9bLNzMxMTZkypd63A6DxsPu7AABNU0hIiLZs2eIZzsrK0rhx49SsWTPFxcX5sbKaO3jwoDp06ODTbWZnZysjI8On2wTQsBHqADQIbdq00dSpU/Xyyy8rLi5OhYWFmjNnjo4ePSqbzabbbrtNjz76qOx2uw4dOqSnn35aFy5ckMPhUHJysmJjY3X99dcrLS1NERERkuQZPn78uBYtWqSoqCgdP35czZo105QpU/T6668rIyNDAwYM0KxZsyRJu3bt0sqVK+V0OhUSEqLp06era9euWrZsmbKyspSXl6esrCxFRERo8eLFOnz4sHbt2qV9+/YpJCREY8aMueQ+rly5Ujt27JDb7VabNm00e/ZsRUdH69e//rW6dOmiTz/9VKdOndIvf/lLPffccwoICNA777yj1atXKyQkRN27d9fatWuVnp6ulJQU5eTkaMKECZozZ45cLpeeeOIJpaen69y5c0pOTm504RjAj2QAwMcyMzNNly5dqow/duyYiYmJMcYYk5ycbJ566injdrtNaWmpGT9+vFm1apUpKyszPXv2NLt37zbGGJOenm4SEhKMy+UyHTt2NPn5+Z71fT/88ccfm06dOpnPP//cGGPMhAkTzIgRI0xpaanJz883N954o/nPf/5jMjIyTEJCgikoKPDU07NnT3P+/HmzdOlS069fP1NYWGiMMWby5MnmhRdeMMYYM336dPPSSy9ddj83bdpkHnnkEeN0Oo0xxmzYsMFMnDjRGGPM2LFjzdSpU43L5TKFhYWmV69eJi0tzRw/ftzExsaaU6dOGWOMWbZsmenYsaMxxpiPP/7YxMfHe7bTsWNHs337dmOMMTt27DD9+vWrzf8aAI0YZ+oANBg2m00hISGSpI8++kh//etfZbPZFBQUpJEjR+q1115Tz549FRAQoNtvv12S9LOf/Uxbt271uu62bduqc+fOkqSrr75a4eHhCgoKUkREhJo3b66zZ8/qwIEDys3N1bhx4yrV9M0330iSbrnlFoWFhUmSOnfurLNnz1Z733bv3q309HQNGzZMkuR2u3XhwgXP9DvuuEMBAQEKCwtTu3btdPbsWR09elQ9e/ZUq1atJEljx47VsmXLLrp+h8PhOTN3ww03KD8/v9q1AbAGQh2ABiM9PV0dO3aU9F3oqcjtdqu8vFyBgYGy2WyVph07dkzXXnttpXFlZWWVhoOCgioN2+1VD39ut1uxsbFasmSJZ9ypU6cUFRWlnTt3egKn9F3YMzV4dbbb7dbEiRM1evRoT30VQ+HF1h0YGFhpG4GBgZdcv8PhqLQ8gKaHp18BNAgZGRl68cUXNX78eElSr169tH79ehljVFZWptTUVPXo0UPXXnutbDab9u3bJ0n6/PPPdd9998ntdisiIkLp6emSpJ07d9a4hu7du2vfvn366quvJEl79uzRkCFDVFpaetnlAgMDVV5eftl5evXqpY0bN6qoqEiS9MILLyg5OdnrMmlpacrJyZH03VO2FbfpdDq97hOApoMzdQD8oqSkRElJSZKkgIAABQcH69FHH/VcVk1JSdHTTz+txMREOZ1O3XbbbXrwwQcVFBSkZcuW6dlnn9WCBQvkcDi0bNkyBQUFKSUlRXPnzlWLFi3Uo0cPRUZG1qimDh06aO7cuXr00UdljJHdbtfKlSsVGhp62eV69+6tp556SpI0efLki85z9913KycnR/fcc49sNptat26t+fPnX3a911xzjWbOnKkJEyYoKChInTp1UrNmzTy1BgYGavjw4Vq8eHGN9hOANdlMTa4fAAB8JjMzU1u2bNFvfvMbBQQEaMeOHVqzZk2lM3YA8D3O1AFAA9WqVSvl5uYqMTFRgYGBCg8P17PPPuvvsgA0UJypAwAAsAAelAAAALAAQh0AAIAFEOoAAAAswJIPSnz77Xm53Y3rVsGWLcOUn1/k7zIaNHrkHT3yjh55R4+8o0fVQ58uLyDApiuvbF5n67NkqHO7TaMLdZIaZc2+Ro+8o0fe0SPv6JF39Kh66JPvcPkVAADAAgh1AAAAFkCoAwAAsABCHQAAgAUQ6gAAACyAUAcAAGABhDoAAAALINQBAABYAKEOAADAAgh1AAAAFmDJ14QBkMrdUqmz3N9lNDimoFjFpTXvS7DDLjv/DAbQgBHqAIsqdZbrwBc5/i6jwQkPC1FhUUmNl+vWKVr2YA6ZABou/t0JAABgAYQ6AAAACyDUAQAAWAChDgAAwAIIdQAAABZAqAMAALAAQh0AAIAFEOoAAAAsgFAHAABgAYQ6AAAACyDUAQAAWAChDgAAwAIIdQAAABZAqAMAALAAQh0AAIAFEOoAAAAswF7fGygqKtLIkSP15z//WV999ZUWLVrkmZaTk6OYmBitWrWq0jKbN2/WwoUL1bJlS0nS7bffrmnTptV3qQAAAI1WvYa6Q4cOKSUlRSdOnJAk9enTR3369JEk5eXladSoUZo5c2aV5dLT0zVjxgwlJCTUZ3kAAACWUa+XX1NTUzV79mxFRUVVmbZgwQKNHDlS7du3rzItPT1dmzdv1pAhQ/SHP/xBZ8+erc8yAQAAGj2bMcbU90b69u2rtWvXqm3btpKkEydO6L777tPOnTsVFBRUZf6HH35YDzzwgH7xi19o0aJFys7O1p/+9Kf6LhOwlNyCYn36Za6/y7CMm66PUlREqL/LAIBLqvd76i7mzTff1OjRoy8a6CRpxYoVnj9PnDhRd955Z43Wn59fJLe73rNqnYqMDFdeXqG/y2jQ6JF3FXtUXFquwqISP1fU8ISHhdSqL8XFpcpzueqhooaH75p39Kh66NPlBQTY1LJlWN2tr87WVAMffPCBBg8efNFphYWFevXVVz3DxhjZ7X7JngAAAI2Gz0NdQUGBSkpKdNVVV110emhoqF566SUdOnRIkrRu3Tr179/flyUCAAA0Oj4/BXby5Em1atWqyvjHHntMffv2Vb9+/bRkyRI9+eSTKikpUfv27bVgwQJflwkAANCo+ORBCV/jnjprokfeVezR+dJyHfgix88VNTy1vaeuW6doNQ9uGreC8F3zjh5VD326PEvcUwcAAIC6RagDAACwAEIdAACABRDqAAAALIBQBwAAYAGEOgAAAAsg1AEAAFgAoQ4AAMACCHUAAAAWQKgDAACwAEIdAACABRDqAAAALIBQBwAAYAGEOgAAAAsg1AEAAFgAoQ4AAMACCHUAAAAWQKgDAACwAEIdAACABRDqAAAALIBQBwAAYAGEOgAAAAsg1AEAAFgAoQ4AAMACCHUAAAAWUO+hrqioSAkJCTp58qQkaebMmRowYICSkpKUlJSknTt3Vlnmiy++0LBhwxQXF6fHHntM5eXl9V0mAABAo1avoe7QoUMaNWqUTpw44Rl35MgRrVu3Tlu2bNGWLVvUv3//Ksv98Y9/1OOPP673339fxhilpqbWZ5kAAACNXr2GutTUVM2ePVtRUVGSpOLiYmVnZ+vxxx9XYmKili5dKrfbXWmZrKwslZSUqEuXLpKkoUOHavv27fVZJgAAQKNnr8+VP/PMM5WG8/Pz1b17d82dO1ehoaGaPHmyNm7cqHvuucczT25uriIjIz3DkZGRysnJqdF2W7YM+3GF+0lkZLi/S2jw6JF33/fIFBQrPCzEz9U0TLXpS1CwQyawadyGnFtQLAUG1vt2moXYFR4aVO/bqS8cj6qHPvlOvYa6H7rqqqu0YsUKz/Cvf/1rbd68uVKoM8ZUWc5ms9VoO/n5RXK7q66nIYuMDFdeXqG/y2jQ6JF3FXtUXFquwqISP1fU8ISHhdSqL0XFpTp0LK8eKmp4atujmurWKVol50vrfTv1geNR9dCnywsIsNXpiSif/rPzyy+/1Pvvv+8ZNsbIbq+cK6Ojo3X69GnPcF5enufyLQAAAC7Op6HOGKNnn31WZ8+eldPp1JtvvlnlQYk2bdooODhYBw8elCRt3rxZvXv39mWZAAAAjY5PQ90NN9ygBx54QKNGjVJ8fLw6deqkhIQESdKkSZOUnp4uSVq4cKHmzZunQYMG6cKFC7r33nt9WSYAAECjYzMXu4mtkeOeOmuiR95V7NH50nId+KJmDxk1BbW9XyymYyT31NWxbp2i1TzYp7d21xmOR9VDny6vUd9TBwAAgPpBqAMAALAAQh0AAIAFEOoAAAAsgFAHAABgAYQ6AAAACyDUAQAAWAChDgAAwAIIdQAAABZAqAMAALAAQh0AAIAFEOoAAAAsgFAHAABgAYQ6AAAACyDUAQAAWAChDgAAwAIIdQAAABZAqAMAALAAQh0AAIAFEOoAAAAsgFAHAABgAYQ6AAAACyDUAQAAWIDXUHfhwgX961//kiS99tprmjlzprKzs+u7LgAAANSA11A3c+ZMffDBBzp8+LDWrl2rn/70p3r88cd9URsAAACqyWuoy8zM1O9//3vt3r1bd911l6ZMmaIzZ85UewNFRUVKSEjQyZMnJUlvvvmmEhISlJiYqJkzZ6qsrKzKMps3b1avXr2UlJSkpKQkLV68uPp7BAAA0AR5DXVOp1OStHfvXnXv3l0ul0vFxcXVWvmhQ4c0atQonThxQpKUkZGhl19+WRs2bNDf/vY3ud1uvfHGG1WWS09P14wZM7RlyxZt2bJF06ZNq8EuAQAAND1eQ13Xrl01ePBglZSU6KabbtK4ceMUGxtbrZWnpqZq9uzZioqKkiQFBQXpySefVFhYmGw2mzp27HjR+/PS09O1efNmDRkyRH/4wx909uzZGu4WAABA02IzxpjLzeByufTZZ5/p+uuvV3h4uD788EP16dNHNput2hvp27ev1q5dq7Zt23rGFRQUaPjw4Zo3b55uvfXWSvM//PDDeuCBB/SLX/xCixYtUnZ2tv70pz/VcNeApi23oFiffpnr7zIs4/p2V+rLr7/1dxmWctP1UYqKCPV3GYBl2L3NMHnyZL300kue4dtvv1333HOPUlNTa73RnJwcTZw4UcOGDasS6CRpxYoVnj9PnDhRd955Z43Wn59fJLf7slm1wYmMDFdeXqG/y2jQ6JF3FXtUXFquwqISP1fU8ISHhdSqL05n0+lnbXtUU8XFpcpzuep9O/WB41H10KfLCwiwqWXLsDpb3yVD3dSpU5WRkaHMzEwlJiZ6xpeXlysgoPY/b/fVV19p0qRJGjt2rMaPH19lemFhod5++22NGzdOkmSMkd3uNXsCAAA0aZdMS8nJycrKytLjjz9e6SdMAgMD1aFDh1ptrKioSBMmTNC0adOUlJR00XlCQ0P10ksvqWvXroqJidG6devUv3//Wm0PAACgqbhkqGvbtq3atm2r7du3/6gzcxVt3LhRp0+f1l/+8hf95S9/kfTd/Xa/+93v9Nhjj6lv377q16+flixZoieffFIlJSVq3769FixYUCfbBwAAsCqvD0q89957ev7553X27FkZY2SMkc1m06effuqrGmuMe+qsiR55V7FH50vLdeCLHD9X1PDU9n6xmI6ROnQsrx4qanh8dU9dt07Rah7cOG+v4XhUPfTp8nx2T933lixZohkzZqhz5841euIVAAAAvuM11LVo0UIDBgzwRS0AAACoJa83y8XExGjPnj2+qAUAAAC15PVM3Z49e7Ru3To5HA45HI5GcU8dAABAU+M11L366qs+KAMAAAA/htfLr23atFF6erpSU1MVERGhzz77TG3atPFFbQAAAKgmr6Fu9erV+utf/6rt27erpKREy5cvr/QaLwAAAPif11D397//XWvWrFGzZs105ZVXKjU1Vdu2bfNFbQAAAKgmr6HObrcrKCjIM9yiRQvexQoAANDAeE1nrVu31ocffiibzaaysjK9/PLL3FMHAADQwHgNdY8//riSk5P15ZdfqkuXLoqJidHChQt9URsAAACqyWuoi46O1muvvaYLFy7I5XIpLKzu3lEGAACAuuE11OXl5WnTpk06c+ZMpfHJycn1VRMAAABqyOuDEg899JAOHz4sY0yl/wAAANBweD1T53Q6tXz5cl/UAgAAgFryeqbuxhtv1LFjx3xRCwAAAGrJ65m6m266Sb/61a8UGRlZ6ffpPvjgg3otDAAAANXnNdQtX75cCxcu1NVXX+2LegAAAFALXkPdT37yEw0ePNgXtQAAAKCWvIa622+/Xc8995wGDBhQ6XVhN954Y70WBgAAgOrzGuq2bt0qSXr//fc942w2G/fUAQAANCBeQ92uXbt8UQcAAAB+BK+h7pVXXrno+Pvvv7/OiwEAAEDteA11FX+jrqysTAcPHtStt95ar0UBAACgZryGunnz5lUaLigo4L2vAAAADYzXN0r8UEREhLKysqo1b1FRkRISEnTy5ElJ0v79+5WYmKgBAwZo8eLFF10mOztbY8aM0cCBA/XQQw/p/PnzNS0RAACgyanRPXXGGB05ckQtW7b0uuJDhw4pJSVFJ06ckCSVlJRo1qxZev3119W6dWtNnjxZe/bsUZ8+fSotN2fOHI0ePVrx8fFasWKFXnzxRf3xj3+s4W4BAAA0LV7P1B07dszz3/Hjx9W6dWstXLjQ64pTU1M1e/ZsRUVFSZIOHz6sdu3a6aqrrpLdbldiYqK2b99eaRmn06kDBw4oLi5OkjR06NAq8wAAAKCqat1Td+DAAXXr1k1nzpzRJ598olatWnld8TPPPFNpODc3V5GRkZ7hqKgo5eTkVJrn22+/VVhYmOcds5GRkVXmqY6WLcNqvExDEBkZ7u8SGjx65N33PTIFxQoPC/FzNQ1TbfricNibVD99sa9BwQ6ZwBrfBdQg5BYUS4GB/i6jkmYhdoWHBnmf0cc4bvuO11C3ePFiffrpp3r99ddVUlKi1atX69ixY/rNb35Tow0ZY6qMs9lsNZ6nOvLzi+R2V11XQxYZGa68vEJ/l9Gg0SPvKvaouLRchUUlfq6o4QkPC6lVX5zOptPP2vaopoqKS3XoWF69b6c++KpHNdGtU7RKzpf6u4xKOG5fXkCArU5PRHn9J9IHH3ygv/zlL5KkVq1aad26dXr33XdrvKHo6GidPn3aM5ybm+u5NPu9iIgIFRUVyeVySZLy8vKqzAMAAICqvIY6p9Mph8PhGXY4HLU6exYTE6OMjAx9/fXXcrlc2rZtm3r37l1pHofDoZtvvtkTGjdv3lxlHgAAAFTl9fLrTTfdpN///vcaPny4bDabNm/erJiYmBpvKDg4WPPnz9eUKVNUWlqqPn36aODAgZKkxx57TH379lW/fv00e/ZszZgxQytXrlTr1q21aNGimu8VAABAE2MzF7uRrYLi4mK98MILSktLk91uV2xsrH7729+qWbNmvqqxxrinzprokXcVe3S+tFwHvqj5g0ZWV9t7oWI6Rjba+79qylf3izXmnjbUe+qaB3s9V+NTHLcvr67vqfP6fz80NFQzZ87UyZMn5XK51K5duzrbOAAAAOqG11B34sQJPfzww8rNzZUxRldccYVWrVql6667zhf1AQAAoBq8Pijx1FNPaeLEiTpw4IA++eQTPfTQQ5ozZ44vagMAAEA1eQ11+fn5uuuuuzzDw4YN07fffluvRQEAAKBmvIY6l8ulM2fOeIYLCgrqsx4AAADUgtd76saOHasRI0Zo0KBBkqT33ntP9913X70XBgAAgOrzGupGjBihq6++Wnv37pXb7dbs2bPVo0cPX9QGAACAarpkqMvOzvb8uV27dpV+yiQ7O1s//elP67cyAAAAVNslQ118fLxsNpuMMSopKVHz5s0VGBioc+fOqWXLltq7d68v6wQAAMBlXDLUffbZZ5KkJ554Qrfeeqvi4+MlSR988IH++7//2zfVAQAAoFq8Pv165MgRT6CTpH79+uno0aP1WhQAAABqxmuoc7vd+sc//uEZ/uijj2Sz2eq1KAAAANSM16dfU1JS9Mgjj8jhcMgYI2OMVqxY4YvaAAAAUE1eQ93NN9+s3bt369ixY7LZbOrYsaPsdq+LAQAAwIeqlc4cDoduvPHG+q4FAAAAteT1njoAAAA0fJcMdYcOHfJlHQAAAPgRLhnqZs+eLUm85xUAAKARuOQ9dS6XS+PHj9f//u//6sEHH6wy/c9//nO9FgYAAIDqu2SoW7NmjT7++GNlZGQoLi7OlzUBAACghi4Z6lq1aqVf/epXat26tW699VZlZWWpvLxc7dq182V9AAAAqAavP2kSHR2t+Ph45ebmyu1268orr9SqVat03XXX+aI+AABQDbYAm86Xlvu7jEpMQbGKG1hNNRHssMveiH4nxGuoe+qppzRx4kTdddddkqS3335bc+bM0dq1a+u9OAAAUD2lTpcOHcvzdxmVhIeFqLCoxN9l1Fq3TtGyBzeeFy54zZ/5+fmeQCdJw4YN07fffluvRQEAAKBmvIY6l8ulM2fOeIYLCgrqsx4AAADUgtdzimPHjtWIESM0aNAgSdJ77733o3677q233tK6des8wydPnlRSUpKeeOIJz7jly5fr7bffVosWLSRJ99xzj8aMGVPrbQIAAFid11A3YsQIXX311dq7d6/cbrdmz56tHj161HqDd999t+6++25J0vHjx/Xwww/rt7/9baV5jhw5okWLFqlr16613g4AAEBTUq27/2JjYxUbG1vnG3/yySc1bdo0RUREVBp/5MgRrVmzRpmZmerWrZumT5+u4ODgOt8+AACAVfjtkY79+/erpKTEc1n3e+fPn1enTp00ffp0tWnTRjNmzNCLL76oadOmVXvdLVuG1XW5PhEZGe7vEho8euTd9z0yBcUKDwvxczUNU2364nDYm1Q/fbGvjb2nDa32htrPhlhTdYWGBisyItTfZVSb30Ldhg0bdP/991cZ37x5c61Zs8YzPH78eM2aNatGoS4/v0hut6mTOn0lMjJceXmF/i6jQaNH3lXsUXFpeaP+KYH6UtufWHA6m04/ffUzFI25pw3xpzoaYj8bYp9qori4VHkuV72tPyDAVqcnorw+/frGG29c9M8/RllZmQ4cOKC+fftWmZadna2NGzd6ho0xstsbz2/EAAAA+MMlQ11cXJySk5P1yiuv6OjRo3I6nXrrrbfqZKNffvml2rdvr9DQqqc0Q0JC9PzzzyszM1PGGK1fv179+/evk+0CAABY1SVD3bZt2zR8+HAVFRVpxYoVSkxM1IkTJ/TMM89o586dP2qjmZmZatWqVaVxkyZNUnp6uiIiIjR37lw99NBDGjhwoIwxF71MCwAAgP/vktc1T548qVtuuUXR0dFatmyZJCkxMVG33nqrDh48+KPOng0ePFiDBw+uNK7ifXRxcXGKi4ur9foBAACamkuGumeeeUaZmZk6d+6cVq9erc6dO0uS7rzzTt15550+KxAAAADeXfLy60svvaS///3vat68ucLDw7Vz505lZmYqISGh0tsfAAAA4H+XfazUbrfr2muv1ahRoyRJp06d0pIlS/Svf/3LF7UBAACgmrz+Vsjq1aur/PnHvCYMAAAAdc/r79QBAACg4SPUAQAAWAChDgAAwAIIdQAAABZAqAMAALAAQh0AAIAFEOoAAAAsgFAHAABgAYQ6AAAACyDUAQAAWAChDgAAwAK8vvsV8IVyt1TqLL/sPKagWMWll5+nqavYI7fxczEAAJ8i1KFBKHWW68AXOZedJzwsRIVFJT6qqHGq2KOYjpF+rgYA4EtcfgUAALAAQh0AAIAFEOoAAAAsgFAHAABgAYQ6AAAACyDUAQAAWAChDgAAwAIIdQAAABbglx8fvvfee5Wfny+7/bvNz507VzExMZ7p+/fv17x581RaWqpBgwZp2rRp/igTAACg0fB5qDPG6N///rc+/PBDT6irqKSkRLNmzdLrr7+u1q1ba/LkydqzZ4/69Onj61IBAAAaDZ9ffv33v/8tm82mSZMmaciQIVq3bl2l6YcPH1a7du101VVXyW63KzExUdu3b/d1mQAAAI2Kz8/UnTt3TrGxsXryySdVUlKie++9V9dcc4169uwpScrNzVVk5P9/Z2VUVJRyci7/TtAfatkyrE5r9pXIyHB/l+A3pqBY4WEhXuerzjxN3fc9cjjs9OsSatOXptZPX+xrY+9pQ6u9ofazIdZUXaGhwYqMCPV3GdXm81DXtWtXde3aVZIUGhqq4cOHa8+ePZ5QZ4ypsozNZqvRNvLzi+R2V11PQxYZGa68vEJ/l+E3xaXlnhfRX0rFl9Xj4ir2yOn03tOmqLafo6bUT1991xpzTxvi8agh9rMh9qkmiotLledy1dv6AwJsdXoiyueXXz/55BOlpaV5ho0xle6ti46O1unTpz3Dubm5ioqK8mmNAAAAjY3PQ11hYaEWLFig0tJSFRUVadOmTerfv79nekxMjDIyMvT111/L5XJp27Zt6t27t6/LBAAAaFR8fvn1jjvu0KFDh/SrX/1Kbrdbo0ePVteuXZWUlKTVq1crOjpa8+fP15QpU1RaWqo+ffpo4MCBvi4TAACgUfHL79Q98sgjeuSRRyqN27Jli+fPsbGx+tvf/ubjqgAAABov3igBAABgAYQ6AAAACyDUAQAAWAChDgAAwAIIdQAAABZAqAMAALAAQh0AAIAFEOoAAAAsgFAHAABgAYQ6AAAACyDUAQAAWAChDgAAwAIIdQAAABZAqAMAALAAQh0AAIAFEOoAAAAsgFAHAABgAYQ6AAAACyDUAQAAWAChDgAAwAIIdQAAABZAqAMAALAAQh0AAIAFEOoAAAAsgFAHAABgAXZ/bHT58uV67733JEl9+vRRcnJylelvv/22WrRoIUm65557NGbMGJ/XCQAA0Fj4PNTt379fe/fu1aZNm2Sz2TRx4kTt3LlT/fv398xz5MgRLVq0SF27dvV1eQAAAI2Sz0NdZGSkZsyYoaCgIEnSddddp+zs7ErzHDlyRGvWrFFmZqa6deum6dOnKzg42NelAgAANBo+D3UdOnTw/PnEiRN69913tWHDBs+48+fPq1OnTpo+fbratGmjGTNm6MUXX9S0adOqvY2WLcPqtGZfiYwM93cJfmMKihUeFuJ1vurM09R93yOHw06/LqE2fWlq/fTFvjb2nja02htqPxtiTdUVGhqsyIhQf5dRbX65p06Sjh8/rsmTJ2v69Olq3769Z3zz5s21Zs0az/D48eM1a9asGoW6/Pwiud2mLsutd5GR4crLK/R3GX5TXFquwqKSy84THhbidZ6mrmKPnE7vPW2Kavs5akr99NV3rTH3tCEejxpiPxtin2qiuLhUeS5Xva0/IMBWpyei/PL068GDBzVu3Dj9/ve/11133VVpWnZ2tjZu3OgZNsbIbvdb9gQAAGgUfB7qTp06pYcfflgLFy5UfHx8lekhISF6/vnnlZmZKWOM1q9fX+khCgAAAFTl81NgL7/8skpLSzV//nzPuJEjR2rXrl2aOnWqfv7zn2vu3Ll66KGH5HQ6ddNNN+n+++/3dZkAAACNis9DXUpKilJSUqqMHzVqlOfPcXFxiouL82VZAAAAjRpvlAAAALAAnkCopXK3VOosr7P1mYJiFZfW3foam0b2sDIAAA0Ooa6WSp3lOvBFTp2tr7E/9v1jxXSM9HcJAAA0alx+BQAAsABCHQAAgAUQ6gAAACyAUAcAAGABhDoAAAALINQBAABYAKEOAADAAgh1AAAAFkCoAwAAsABCHQAAgAUQ6gAAACyAUAcAAGABhDoAAAALINQBAABYAKEOAADAAgh1AAAAFkCoAwAAsABCHQAAgAUQ6gAAACyAUAcAAGABhDoAAAALINQBAABYgF9C3datWzV48GD1799f69evrzL9iy++0LBhwxQXF6fHHntM5eXlfqgSAACg8fB5qMvJydHixYv1xhtvaMuWLXrzzTf1f//3f5Xm+eMf/6jHH39c77//vowxSk1N9XWZAAAAjYrd1xvcv3+/unfvriuuuEKSFBcXp+3bt+u3v/2tJCkrK0slJSXq0qWLJGno0KFaunSpRo8eXe1tBATY6rrsKuyBAQoNcdTZ+poF2+Uqr7v1NTbV6WdT71F1VOxRXX9GraK2n6Om1E9ffdcac08b4vGoIfazIfapJuyBAfWaKep63T4Pdbm5uYqMjPQMR0VF6fDhw5ecHhkZqZycnBpt48orm//4Qquhbeuf+GQ7TcW1ba/0dwmWQ0/rFv2se/S0btHPps3nl1+NMVXG2Wy2ak8HAABAVT4PddHR0Tp9+rRnODc3V1FRUZecnpeXV2k6AAAAqvJ5qOvRo4fS0tJUUFCgCxcuaMeOHerdu7dneps2bRQcHKyDBw9KkjZv3lxpOgAAAKqymYtd76xnW7du1apVq+R0OjV8+HBNmjRJkyZN0tSpU/Xzn/9cR48eVUpKis6fP6/OnTtr3rx5CgoK8nWZAAAAjYZfQh0AAADqFm+UAAAAsABCHQAAgAUQ6gAAACyAUAcAAGABhDoAAAALINTVs6KiIiUkJOjkyZOSvnv3bWJiogYMGKDFixd75vviiy80bNgwxcXF6bHHHlN5ebkkKTs7W2PGjNHAgQP10EMP6fz5837Zj/qyfPlyxcfHKz4+XgsWLJBEj37ohRde0ODBgxUfH69XXnlFEj26lOeee04zZsyQVPNenDt3Tg888IAGDRqkMWPGKC8vz2/7UV/uvfdexcfHKykpSUlJSTp06JC2bt2qwYMHq3///lq/fr1n3pp+xqxi165dGjp0qAYOHKinn35aEt+3it566y3P5ycpKUm//OUvNXfuXHr0A1u2bPH83fbcc89J8tExyaDe/Otf/zIJCQnmxhtvNJmZmebChQumT58+5ptvvjFOp9OMHz/efPjhh8YYY+Lj481nn31mjDFm5syZZv369cYYYx544AGzbds2Y4wxy5cvNwsWLPDLvtSHffv2mREjRpjS0lJTVlZm7r33XrN161Z6VME//vEPM3LkSON0Os2FCxfMHXfcYb744gt6dBH79+83t956q5k+fboxpua9mDNnjlm1apUxxphNmzaZ3/3ud77dgXrmdrtNz549jdPp9Iz7z3/+Y+644w7z7bffmvPnz5vExERz/PjxWh2rrOCbb74xvXr1MqdOnTJlZWVm1KhR5sMPP+T7dgnHjh0z/fv3N9nZ2fSoguLiYtOtWzeTn59vnE6nGT58uNm3b59PjkmcqatHqampmj17tuc1Z4cPH1a7du101VVXyW63KzExUdu3b1dWVpZKSkrUpUsXSdLQoUO1fft2OZ1OHThwQHFxcZXGW0VkZKRmzJihoKAgORwOXXfddTpx4gQ9quCWW27R2rVrZbfblZ+fL5fLpXPnztGjHzhz5owWL16sBx98UJJq1YsPP/xQiYmJkqSEhAR99NFHcjqdvt+ZevLvf/9bNptNkyZN0pAhQ7Ru3Trt379f3bt31xVXXKHQ0FDFxcVp+/btNT5WWcXOnTs1ePBgtWrVSg6HQ4sXL1azZs34vl3Ck08+qWnTpikzM5MeVeByueR2u3XhwgWVl5ervLxcdrvdJ8ckQl09euaZZ3TzzTd7hnNzcxUZGekZjoqKUk5OTpXxkZGRysnJ0bfffquwsDDZ7fZK462iQ4cOng/4iRMn9O6778pms9GjH3A4HFq6dKni4+MVGxvL5+ginnjiCU2bNk0tWrSQVPW7Vp1eVFzGbrcrLCxMBQUFPt6T+nPu3DnFxsZqxYoVevXVV7VhwwZlZ2dX67Pk7TNmFV9//bVcLpcmTJigIUOG6I033uD7dgn79+9XSUmJBg0aRI9+ICwsTL/73e80aNAg9e7dW23atJHD4fDJMYlQ50PmIi/vsNlsNR5vNcePH9f48eM1ffp0XX311VWm0yNp6tSpSktL06lTp3TixIkq05tyj9566y21bt1asbGxnnF11YuAAOscIrt27aoFCxYoNDRUERERGj58uJYuXVplvqb8WXK5XEpLS9Pzzz+v1NRUpaene+6Hrqgp9+h7GzZs0P333y+Jv9t+6OjRo3r77be1e/du7d27VwEBAdq3b1+V+erjmGSvebmorejoaJ0+fdoznJubq6ioqCrj8/LyFBUVpYiICBUVFcnlcikwMNAz3koOHjyoqVOnatasWYqPj9c///lPelTBV199pbKyMnXq1EnNmjXTgAEDtH37dgUGBnrmaeo9evfdd5WXl6ekpCSdPXtWxcXFstlsNe5FVFSUTp8+rVatWqm8vFxFRUW64oor/LRXde+TTz6R0+n0hF9jjNq0aVOt75u3z5hV/Nd//ZdiY2MVEREhSerXrx/ft4soKyvTgQMHNH/+fEn83fZDe/fuVWxsrFq2bCnpu0uqL7/8sk+OSdb5Z2gjEBMTo4yMDM8p/m3btnlOzQYHB+vgwYOSpM2bN6t3795yOBy6+eab9e6771YabxWnTp3Sww8/rIULFyo+Pl4SPfqhkydPKiUlRWVlZSorK9MHH3ygkSNH0qMKXnnlFW3btk1btmzR1KlT1bdvX82bN6/GvejTp482b94s6bugePPNN8vhcPhln+pDYWGhFixYoNLSUhUVFWnTpk16/vnnlZaWpoKCAl24cEE7duxQ7969a/w9tIo77rhDe/fu1blz5+RyufQ///M/GjhwIN+3H/jyyy/Vvn17hYaGSuK4/UM33HCD9u/fr+LiYhljtGvXLt1yyy0+OSbZzMXO/aFO9e3bV2vXrlXbtm2VlpamefPmqbS0VH369NHMmTNls9l09OhRpaSk6Pz58+rcubPmzZunoKAgZWVlacaMGcrPz1fr1q21aNEi/eQnP/H3LtWJp59+Wm+//XalS64jR45U+/bt6VEFS5cu9ZwtGDBggKZMmcLn6BLeeecd/fOf/9T8+fNr3IszZ85oxowZyszMVHh4uBYuXKi2bdv6e5fq1JIlS/T+++/L7XZr9OjRuu+++7R161atWrVKTqdTw4cP16RJkySpxp8xq9i4caNeffVVOZ1O9ezZUykpKfrHP/7B962Cd999Vzt37qz00yUckypbvXq13nnnHTkcDv385z/X7NmzlZGRUe/HJEIdAACABXD5FQAAwAIIdQAAABZAqAMAALAAQh0AAIAFEOoAAAAsgFAHwGdOnjypTp06KSkpSUlJSUpMTNTQoUM9v8XU2Lz11ltav379Raddf/31PnnNWGZmpqZMmSLpu/527dq13rcJoGHijRIAfCokJERbtmzxDGdlZWncuHFq1qyZ56XWjcXBgwfVoUMHv9aQnZ2tjIwMv9YAoGEg1AHwqzZt2mjq1Kl6+eWXFRcXp8LCQs2ZM0dHjx6VzWbTbbfdpkcffVR2u12HDh3S008/rQsXLsjhcCg5OVmxsbG6/vrrlZaW5nm90/fDx48f16JFixQVFaXjx4+rWbNmmjJlil5//XVlZGRowIABmjVrliRp165dWrlypZxOp0JCQjR9+nR17dpVy5YtU1ZWlvLy8pSVlaWIiAgtXrxYhw8f1q5du7Rv3z6FhIRozJgx1d7nlStXaseOHXK73WrTpo1mz56t6Oho/frXv1aXLl306aef6tSpU/rlL3+p5557TgEBAXrnnXe0evVqhYSEqHv37lq7dq3S09OVkpKinJwcTZgwQXPmzJHL5dITTzyh9PR0nTt3TsnJyY0uLAOoJQMAPpKZmWm6dOlSZfyxY8dMTEyMMcaY5ORk89RTTxm3221KS0vN+PHjzapVq0xZWZnp2bOn2b17tzHGmPT0dJOQkGBcLpfp2LGjyc/P96zv++GPP/7YdOrUyXz++efGGGMmTJhgRowYYUpLS01+fr658cYbzX/+8x+TkZFhEhISTEFBgaeenj17mvPnz5ulS5eafv36mcLCQmOMMZMnTzYvvPCCMcaY6dOnm5deeumi+/rDmr63adMm88gjjxin02mMMWbDhg1m4sSJxhhjxo4da6ZOnWpcLpcpLCw0vXr1Mmlpaeb48eMmNjbWnDp1yhhjzLJly0zHjh2NMcZ8/PHHJj4+3tPfjh07mu3btxtjjNmxY4fp169fdf7XALAAztQB8DubzaaQkBBJ0kcffaS//vWvstlsCgoK0siRI/Xaa6+pZ8+eCggI0O233y5J+tnPfqatW7d6XXfbtm3VuXNnSdLVV1+t8PBwBQUFKSIiQs2bN9fZs2d14MAB5ebmaty4cZVq+uabbyRJt9xyi8LCwiRJnTt31tmzZ2u9r7t371Z6erqGDRsmSXK73bpw4YJn+h133KGAgACFhYWpXbt2Onv2rI4ePaqePXuqVatWkqSxY8dq2bJlF12/w+HwnJm74YYblJ+fX+taATQuhDoAfpeenq6OHTtK+i7kVOR2u1VeXq7AwEDZbLZK044dO6Zrr7220riysrJKwz98L6ndXvWw53a7FRsbqyVLlnjGnTp1SlFRUdq5c6cncErfhT3zI96u6Ha7NXHiRI0ePdpTb8WQeLFtBQYGVtpmYGDgJddf8YXfP+wXAGvj6VcAfpWRkaEXX3xR48ePlyT16tVL69evlzFGZWVlSk1NVY8ePXTttdfKZrNp3759kqTPP/9c9913n9xutyIiIpSeni5J2rlzZ41r6N69u/bt26evvvpKkrRnzx4NGTJEpaWll10uMDBQ5eXlNdpWr169tHHjRhUVFUmSXnjhBSUnJ3tdJi0tTTk5OZK+e+q2Yg1Op7NGNQCwJs7UAfCpkpISJSUlSZICAgIUHBysRx991HNZNSUlRU8//bQSExPldDp122236cEHH1RQUJCWLVumZ599VgsWLJDD4dCyZcsUFBSklJQUzZ07Vy1atFCPHj0UGRlZo5o6dOiguXPn6tFHH5UxRna7XStXrlRoaOhll+vdu7eeeuopSdLkyZOrTO/Xr1+l4UWLFunuu+9WTk6O7rnnHtlsNrVu3Vrz58+/7HauueYazZw5UxMmTFBQUJA6deqkZs2aeWoPDAzU8OHDtXjx4prsNgCLsZkfcx0BAFDvMjMztWXLFv3mN79RQECAduzYoTVr1lQ6YwcAnKkDgAauVatWys3NVWJiogIDAxUeHq5nn33W32UBaGA4UwcAAGABPCgBAABgAYQ6AAAACyDUAQAAWAChDgAAwAIIdQAAABbw/wBAiGTqY1e7PgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10,5)\n",
    "\n",
    "lengths = [min(l, 512) for l in length]\n",
    "sns.distplot(length, kde = False, rug = False)\n",
    "plt.title('Document length')\n",
    "plt.xlabel('Document Length')\n",
    "plt.ylabel('# of documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "398bd4e3-51e0-4c6a-9081-e99f9cb6c6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token: [PAD], ID : 0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 512\n",
    "print('Padding token: {:}, ID : {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen = MAX_LEN, dtype = 'long', value = 0, truncating = 'post', padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b42e4305-7683-4044-bc0e-2e61bb712fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "for sent in input_ids:\n",
    "    att_mask = [int(token_id>0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70febe73-540e-4a71-9780-f971ce5764fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train[1].tolist()\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, label, random_state = 0, test_size = 0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, label, random_state = 0, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a69e7c13-d6ae-4270-85a6-c264e8c13938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2448/1696369575.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_inputs = torch.tensor(train_inputs)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2448/1696369575.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_inputs = torch.tensor(validation_inputs)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2448/1696369575.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2448/1696369575.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_labels = torch.tensor(validation_labels)\n"
     ]
    }
   ],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91eab5a9-49c5-4d5a-bd2f-d9a84f0b5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "batch_size = 1\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edde7ab8-f75c-458c-ab2e-684b98594fc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcecf4717bf442f6b30c6c32a1040a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels = 4, output_attentions = False, output_hidden_states = False)\n",
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "343280c1-7da4-4432-ac00-153c6f56305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "291965ac-eee2-4e91-a9cf-07794f7d970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "total_steps = len(train_dataloader)*epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1b6649c1-9eb0-41d5-a32a-a65534deb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(prds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4dde5c23-2294-4613-b1df-c0c02e23aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    \n",
    "    return str(datetime.timedelta(seconds=elapsed_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "154fd5d2-2c32-4227-ad4f-228b5cad8f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================Epoch 1/5====================\n",
      "Training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.00 GiB total capacity; 9.73 GiB already allocated; 0 bytes free; 9.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2448/2560832888.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1538\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1540\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1541\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    997\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m         )\n\u001b[1;32m--> 999\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1000\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    583\u001b[0m                 )\n\u001b[0;32m    584\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    473\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     ):\n\u001b[1;32m--> 402\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;31m# Mask heads if we want to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[1;34m\"but got {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.00 GiB total capacity; 9.73 GiB already allocated; 0 bytes free; 9.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print(\"=================Epoch {:}/{:}====================\".format(epoch_i+1, epochs))\n",
    "    print(\"Training\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 100==0 and not step ==0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5,} of {:>5,}. Elapsed: {:}'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask, labels = b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_loss/len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    # validation\n",
    "    print(\"\")\n",
    "    print(\"Running Validation\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ouputs = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        labels_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps +=1\n",
    "        \n",
    "    print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training Complete!\")       \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a96e3-f47a-4be9-9c0d-8c889cf7b132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c00b4-a8e1-43ed-ae45-04709a0cb25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
